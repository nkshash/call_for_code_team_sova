# -*- coding: utf-8 -*-
"""googlenet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1440A65OEtkVcOeXfQZf7VBicspsifdYr
"""



dataset_dir = "dataset_path"


class_counts = defaultdict(int)


for class_name in os.listdir(dataset_dir):
    class_dir = os.path.join(dataset_dir, class_name)
    if os.path.isdir(class_dir):
        for img_file in os.listdir(class_dir):
            class_counts[class_name] += 1

for class_name, count in class_counts.items():
    print(f"{class_name}: {count}")

import os
import numpy as np
from PIL import Image


image_sizes = []

for class_name in os.listdir(dataset_dir):
    class_dir = os.path.join(dataset_dir, class_name)
    if os.path.isdir(class_dir):
        for img_file in os.listdir(class_dir):
            img_path = os.path.join(class_dir, img_file)
            with Image.open(img_path) as img:
                width, height = img.size
                image_sizes.append((width, height))


image_sizes = np.array(image_sizes)

median_size = np.median(image_sizes, axis=0)

print(f"Median image size: {median_size}")

import os
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

image_widths = []
image_heights = []

for class_name in os.listdir(dataset_dir):
    class_dir = os.path.join(dataset_dir, class_name)
    if os.path.isdir(class_dir):
        for img_file in os.listdir(class_dir):
            img_path = os.path.join(class_dir, img_file)
            with Image.open(img_path) as img:
                width, height = img.size
                image_widths.append(width)
                image_heights.append(height)

image_widths = np.array(image_widths)
image_heights = np.array(image_heights)

plt.figure(figsize=(8, 6))
plt.scatter(image_widths, image_heights, color='blue', alpha=0.5)
plt.title('Image Sizes Distribution')
plt.xlabel('Width')
plt.ylabel('Height')
plt.grid(True)
plt.show()

## run this part only
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import InceptionV3
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt


image_size = 225
batch_size = 32
validation_split = 0.2

datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest',
    validation_split=validation_split
)

train_generator = datagen.flow_from_directory(
    dataset_dir,
    target_size=(image_size, image_size),
    batch_size=batch_size,
    class_mode='categorical',
    subset='training'
)

validation_generator = datagen.flow_from_directory(
    dataset_dir,
    target_size=(image_size, image_size),
    batch_size=batch_size,
    class_mode='categorical',
    subset='validation'
)

base_model = InceptionV3(weights='imagenet', include_top=False)
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu')(x)
predictions = Dense(train_generator.num_classes, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=predictions)

for layer in base_model.layers:
    layer.trainable = False

model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.n // batch_size,
    epochs=20,
    validation_data=validation_generator,
    validation_steps=validation_generator.n // batch_size
)

# Save the model
model.save("wound_classifier_model_googlenet.h5")

val_loss, val_acc = model.evaluate(validation_generator)
print(f"Validation accuracy: {val_acc * 100:.2f}%")

plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.show()

model.save("wound_classifier_model_googlenet")


with open("classes.txt", "w") as f:
    for cls, idx in train_generator.class_indices.items():
        f.write(f"{cls}: {idx}\n")


#------------- my custom articture as per googlenet paper ---------

import tensorflow as tf
from tensorflow.keras import layers, models, Input

def inception_module(x, filters):
    (branch1, branch2, branch3, branch4) = filters
    branch1x1 = layers.Conv2D(branch1, (1, 1), padding='same', activation='relu')(x)
    branch3x3 = layers.Conv2D(branch2[0], (1, 1), padding='same', activation='relu')(x)
    branch3x3 = layers.Conv2D(branch2[1], (3, 3), padding='same', activation='relu')(branch3x3)
    branch5x5 = layers.Conv2D(branch3[0], (1, 1), padding='same', activation='relu')(x)
    branch5x5 = layers.Conv2D(branch3[1], (5, 5), padding='same', activation='relu')(branch5x5)
    branch_pool = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same')(x)
    branch_pool = layers.Conv2D(branch4, (1, 1), padding='same', activation='relu')(branch_pool)
    outputs = layers.concatenate([branch1x1, branch3x3, branch5x5, branch_pool], axis=-1)
    return outputs

def auxiliary_classifier(x, name=None):
    x = layers.AveragePooling2D((5, 5), strides=3)(x)
    x = layers.Conv2D(128, (1, 1), padding='same', activation='relu')(x)
    x = layers.Flatten()(x)
    x = layers.Dense(1024, activation='relu')(x)
    x = layers.Dropout(0.7)(x)
    x = layers.Dense(1000, activation='softmax', name=name)(x)

    return x

def google_lenet(input_shape=(224, 224, 3)):
    inputs = Input(shape=input_shape)
    x = layers.Conv2D(64, (7, 7), strides=(2, 2), padding='same', activation='relu')(inputs)
    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)
    x = layers.LRN()(x)
    x = layers.Conv2D(192, (3, 3), padding='same', activation='relu')(x)
    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)
    x = layers.LRN()(x)
    x = inception_module(x, (64, (96, 128), (16, 32), 32))
    x = inception_module(x, (128, (128, 192), (32, 96), 64))
    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)
    x = inception_module(x, (192, (96, 208), (16, 48), 64))
    aux1 = auxiliary_classifier(x, name='aux1')
    x = inception_module(x, (160, (112, 224), (24, 64), 64))
    x = inception_module(x, (128, (128, 256), (24, 64), 64))
    x = inception_module(x, (112, (144, 288), (32, 64), 64))
    aux2 = auxiliary_classifier(x, name='aux2')
    x = inception_module(x, (256, (160, 320), (32, 128), 128))
    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)
    x = inception_module(x, (256, (160, 320), (32, 128), 128))
    x = inception_module(x, (384, (192, 384), (48, 128), 128))
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dropout(0.4)(x)
    outputs = layers.Dense(1000, activation='softmax', name='output')(x)

    model = models.Model(inputs=inputs, outputs=[outputs, aux1, aux2])
    return model


model = google_lenet()
model.summary()